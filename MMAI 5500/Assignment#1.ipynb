{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignmengt#1 Yuntong Zhu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 3), (400,), (200, 3), (200,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '/Users/zoezhu/Desktop/assign1_data.csv'\n",
    "data = np.genfromtxt(fname, dtype='float', delimiter=',', skip_header=1)\n",
    "X, y = data[:, :-1], data[:, -1].astype(int)\n",
    "X_train, y_train = X[:400], y[:400]\n",
    "X_test, y_test = X[400:], y[400:]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        Initialize weights & biases.\n",
    "        Weights should be initialized with values drawn from a normal distribution scaled by 0.01.\n",
    "        Biases are initialized to 0.0.\n",
    "        \"\"\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        A forward pass through the layer to give z.\n",
    "        Compute it using np.dot(...) and then add the biases.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dz):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \"\"\"\n",
    "        # Gradients of weights\n",
    "        self.dweights = np.dot(self.inputs.T, dz)\n",
    "        # Gradients of biases\n",
    "        self.dbiases = np.sum(dz, axis=0, keepdims=True)\n",
    "        # Gradients of inputs\n",
    "        self.dinputs = np.dot(dz, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    \"\"\"\n",
    "    ReLu activation\n",
    "    \"\"\"\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        self.z = z\n",
    "        self.activity = np.maximum(0, z)\n",
    "\n",
    "    def backward(self, dactivity):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \"\"\"\n",
    "        self.dz = dactivity.copy()\n",
    "        self.dz[self.z <= 0] = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Softmax forward pass\n",
    "        \"\"\"\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        self.probs = e_z / e_z.sum(axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "\n",
    "    def backward(self, dprobs):\n",
    "        \"\"\"\n",
    "        Softmax backward pass\n",
    "        \"\"\"\n",
    "        # Empty array\n",
    "        self.dz = np.empty_like(dprobs)\n",
    "        for i, (prob, dprob) in enumerate(zip(self.probs, dprobs)):\n",
    "            # Flatten to a column vector\n",
    "            prob = prob.reshape(-1, 1)\n",
    "            # Jacobian matrix\n",
    "            jacobian = np.diagflat(prob) - np.dot(prob, prob.T)\n",
    "            self.dz[i] = np.dot(jacobian, dprob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossentropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, probs, oh_y_true):\n",
    "        \"\"\"\n",
    "        Use one-hot encoded y_true.\n",
    "        \"\"\"\n",
    "        # Clip to prevent division by 0\n",
    "        # Clip both sides to not bias up.\n",
    "        probs_clipped = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "        # Negative log likelihoods\n",
    "        loss = -np.sum(oh_y_true * np.log(probs_clipped), axis=1)\n",
    "        return loss.mean(axis=0)\n",
    "\n",
    "    def backward(self, probs, oh_y_true):\n",
    "        \"\"\"\n",
    "        Use one-hot encoded y_true.\n",
    "        \"\"\"\n",
    "        # Number of examples in batch and number of classes\n",
    "        batch_sz, n_class = probs.shape\n",
    "        # Get the gradient\n",
    "        self.dprobs = -oh_y_true / probs\n",
    "        # Normalize the gradient\n",
    "        self.dprobs = self.dprobs / batch_sz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        # Initialize the optimizer with a learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        # Update weights and biases \n",
    "        layer.weights -= self.learning_rate * layer.dweights\n",
    "        layer.biases -= self.learning_rate * layer.dbiases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert probabilities to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(probs):\n",
    "    \"\"\"\n",
    "    Convert probabilities \n",
    "    \"\"\"\n",
    "    y_preds = np.argmax(probs, axis=1)\n",
    "    return y_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_preds, y_true):\n",
    "    \"\"\"\n",
    "    accuracy\n",
    "    \"\"\"\n",
    "    return np.mean(y_preds == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y_true, n_class):\n",
    "    Oh_y_true = np.eye(n_class)[y_true]\n",
    "    return Oh_y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture: \n",
    "# Forward pass\n",
    "def forward_pass(X, y_true, oh_y_true):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.z)\n",
    "    \n",
    "    dense2.forward(activation1.activity)\n",
    "    activation2.forward(dense2.z)\n",
    "    \n",
    "    dense3.forward(activation2.activity)\n",
    "    probs = output_activation.forward(dense3.z)\n",
    "    \n",
    "    loss = crossentropy.forward(probs, oh_y_true)\n",
    "    return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single backward pass through the entire network\n",
    "def backward_pass(probs, y_true, oh_y_true):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Compute the gradient of loss\n",
    "    crossentropy.backward(probs, oh_y_true)\n",
    "    \n",
    "    output_activation.backward(crossentropy.dprobs)\n",
    "    dense3.backward(output_activation.dz)\n",
    "    \n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dz)\n",
    "    \n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_sz = 32\n",
    "\n",
    "n_inputs = 3\n",
    "n_class = 3\n",
    "\n",
    "# the network layers\n",
    "dense1 = DenseLayer(n_inputs, 4)\n",
    "activation1 = ReLu()\n",
    "dense2 = DenseLayer(4, 8)\n",
    "activation2 = ReLu()\n",
    "dense3 = DenseLayer(8, n_class)\n",
    "output_activation = Softmax()\n",
    "\n",
    "crossentropy = CrossEntropyLoss()\n",
    "optimizer = SGD(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_mini_batch(X, y, batch_sz):\n",
    "    index = np.random.choice(X.shape[0], batch_sz, replace=False)\n",
    "    return X[index], y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Loss: 1.0986, Accuracy: 0.3125\n",
      "Loss: 1.0980, Accuracy: 0.3750\n",
      "Loss: 1.0984, Accuracy: 0.3750\n",
      "Loss: 1.1017, Accuracy: 0.1562\n",
      "Loss: 1.0967, Accuracy: 0.4062\n",
      "Loss: 1.0955, Accuracy: 0.4375\n",
      "Loss: 1.0979, Accuracy: 0.3438\n",
      "Loss: 1.1025, Accuracy: 0.2812\n",
      "Loss: 1.0932, Accuracy: 0.4688\n",
      "Loss: 1.0968, Accuracy: 0.3438\n",
      "Loss: 1.0979, Accuracy: 0.3438\n",
      "Loss: 1.1026, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3400\n",
      "epoch: 1\n",
      "Loss: 1.1018, Accuracy: 0.2812\n",
      "Loss: 1.1045, Accuracy: 0.2500\n",
      "Loss: 1.0984, Accuracy: 0.3438\n",
      "Loss: 1.0972, Accuracy: 0.3750\n",
      "Loss: 1.0940, Accuracy: 0.4375\n",
      "Loss: 1.0982, Accuracy: 0.3750\n",
      "Loss: 1.1065, Accuracy: 0.2188\n",
      "Loss: 1.1049, Accuracy: 0.1875\n",
      "Loss: 1.0970, Accuracy: 0.4688\n",
      "Loss: 1.0994, Accuracy: 0.3438\n",
      "Loss: 1.0966, Accuracy: 0.3438\n",
      "Loss: 1.1088, Accuracy: 0.2188\n",
      "Test Accuracy: 0.3400\n",
      "epoch: 2\n",
      "Loss: 1.0991, Accuracy: 0.2812\n",
      "Loss: 1.1019, Accuracy: 0.3438\n",
      "Loss: 1.0974, Accuracy: 0.4375\n",
      "Loss: 1.1007, Accuracy: 0.3125\n",
      "Loss: 1.1015, Accuracy: 0.2188\n",
      "Loss: 1.0988, Accuracy: 0.2812\n",
      "Loss: 1.0984, Accuracy: 0.3438\n",
      "Loss: 1.1014, Accuracy: 0.2812\n",
      "Loss: 1.0999, Accuracy: 0.1875\n",
      "Loss: 1.0999, Accuracy: 0.2500\n",
      "Loss: 1.0987, Accuracy: 0.3125\n",
      "Loss: 1.0986, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 3\n",
      "Loss: 1.0980, Accuracy: 0.4062\n",
      "Loss: 1.0982, Accuracy: 0.3750\n",
      "Loss: 1.0972, Accuracy: 0.3750\n",
      "Loss: 1.0970, Accuracy: 0.3438\n",
      "Loss: 1.0944, Accuracy: 0.5000\n",
      "Loss: 1.0874, Accuracy: 0.5312\n",
      "Loss: 1.1030, Accuracy: 0.3125\n",
      "Loss: 1.0983, Accuracy: 0.3438\n",
      "Loss: 1.0993, Accuracy: 0.3438\n",
      "Loss: 1.1006, Accuracy: 0.3125\n",
      "Loss: 1.0962, Accuracy: 0.3750\n",
      "Loss: 1.0903, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 4\n",
      "Loss: 1.0981, Accuracy: 0.3438\n",
      "Loss: 1.0927, Accuracy: 0.4062\n",
      "Loss: 1.1029, Accuracy: 0.2812\n",
      "Loss: 1.1007, Accuracy: 0.3438\n",
      "Loss: 1.0940, Accuracy: 0.4062\n",
      "Loss: 1.1109, Accuracy: 0.1875\n",
      "Loss: 1.1097, Accuracy: 0.2500\n",
      "Loss: 1.0982, Accuracy: 0.3438\n",
      "Loss: 1.0921, Accuracy: 0.4375\n",
      "Loss: 1.1001, Accuracy: 0.3438\n",
      "Loss: 1.0957, Accuracy: 0.3750\n",
      "Loss: 1.1038, Accuracy: 0.3750\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 5\n",
      "Loss: 1.0898, Accuracy: 0.4688\n",
      "Loss: 1.1056, Accuracy: 0.2812\n",
      "Loss: 1.1130, Accuracy: 0.1562\n",
      "Loss: 1.1008, Accuracy: 0.3438\n",
      "Loss: 1.0859, Accuracy: 0.4688\n",
      "Loss: 1.0891, Accuracy: 0.4062\n",
      "Loss: 1.0818, Accuracy: 0.5000\n",
      "Loss: 1.1054, Accuracy: 0.3125\n",
      "Loss: 1.0921, Accuracy: 0.4062\n",
      "Loss: 1.0898, Accuracy: 0.4375\n",
      "Loss: 1.1128, Accuracy: 0.2812\n",
      "Loss: 1.0957, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 6\n",
      "Loss: 1.0973, Accuracy: 0.3750\n",
      "Loss: 1.1067, Accuracy: 0.2500\n",
      "Loss: 1.0984, Accuracy: 0.3125\n",
      "Loss: 1.1003, Accuracy: 0.3750\n",
      "Loss: 1.0911, Accuracy: 0.3750\n",
      "Loss: 1.1060, Accuracy: 0.2812\n",
      "Loss: 1.0929, Accuracy: 0.4375\n",
      "Loss: 1.0937, Accuracy: 0.4062\n",
      "Loss: 1.0842, Accuracy: 0.5000\n",
      "Loss: 1.0953, Accuracy: 0.3750\n",
      "Loss: 1.0805, Accuracy: 0.5000\n",
      "Loss: 1.0723, Accuracy: 0.5312\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 7\n",
      "Loss: 1.0958, Accuracy: 0.3750\n",
      "Loss: 1.1128, Accuracy: 0.2812\n",
      "Loss: 1.1099, Accuracy: 0.2812\n",
      "Loss: 1.0996, Accuracy: 0.3438\n",
      "Loss: 1.1037, Accuracy: 0.3125\n",
      "Loss: 1.1039, Accuracy: 0.3125\n",
      "Loss: 1.1074, Accuracy: 0.2812\n",
      "Loss: 1.1021, Accuracy: 0.3125\n",
      "Loss: 1.1022, Accuracy: 0.3125\n",
      "Loss: 1.0901, Accuracy: 0.4375\n",
      "Loss: 1.0967, Accuracy: 0.3438\n",
      "Loss: 1.1079, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 8\n",
      "Loss: 1.1000, Accuracy: 0.3125\n",
      "Loss: 1.1026, Accuracy: 0.2812\n",
      "Loss: 1.0946, Accuracy: 0.3750\n",
      "Loss: 1.1030, Accuracy: 0.3438\n",
      "Loss: 1.1036, Accuracy: 0.3125\n",
      "Loss: 1.0990, Accuracy: 0.3750\n",
      "Loss: 1.1063, Accuracy: 0.2500\n",
      "Loss: 1.0999, Accuracy: 0.3125\n",
      "Loss: 1.0873, Accuracy: 0.5312\n",
      "Loss: 1.0933, Accuracy: 0.4062\n",
      "Loss: 1.0961, Accuracy: 0.3750\n",
      "Loss: 1.0938, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 9\n",
      "Loss: 1.0934, Accuracy: 0.4062\n",
      "Loss: 1.1130, Accuracy: 0.2188\n",
      "Loss: 1.0937, Accuracy: 0.4062\n",
      "Loss: 1.1110, Accuracy: 0.2188\n",
      "Loss: 1.1067, Accuracy: 0.2812\n",
      "Loss: 1.0977, Accuracy: 0.3438\n",
      "Loss: 1.0961, Accuracy: 0.4062\n",
      "Loss: 1.1029, Accuracy: 0.2812\n",
      "Loss: 1.1036, Accuracy: 0.2500\n",
      "Loss: 1.1029, Accuracy: 0.3125\n",
      "Loss: 1.1012, Accuracy: 0.2812\n",
      "Loss: 1.0948, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 10\n",
      "Loss: 1.0983, Accuracy: 0.3750\n",
      "Loss: 1.0998, Accuracy: 0.3438\n",
      "Loss: 1.1047, Accuracy: 0.2188\n",
      "Loss: 1.0933, Accuracy: 0.4688\n",
      "Loss: 1.1060, Accuracy: 0.2188\n",
      "Loss: 1.0950, Accuracy: 0.4062\n",
      "Loss: 1.0999, Accuracy: 0.3438\n",
      "Loss: 1.1001, Accuracy: 0.3438\n",
      "Loss: 1.1031, Accuracy: 0.2500\n",
      "Loss: 1.1004, Accuracy: 0.2812\n",
      "Loss: 1.1010, Accuracy: 0.2500\n",
      "Loss: 1.0996, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 11\n",
      "Loss: 1.0998, Accuracy: 0.3438\n",
      "Loss: 1.1005, Accuracy: 0.2812\n",
      "Loss: 1.0972, Accuracy: 0.4688\n",
      "Loss: 1.1004, Accuracy: 0.2812\n",
      "Loss: 1.0991, Accuracy: 0.4062\n",
      "Loss: 1.0948, Accuracy: 0.4688\n",
      "Loss: 1.0984, Accuracy: 0.3125\n",
      "Loss: 1.1011, Accuracy: 0.3125\n",
      "Loss: 1.0993, Accuracy: 0.2812\n",
      "Loss: 1.1010, Accuracy: 0.2812\n",
      "Loss: 1.1005, Accuracy: 0.2188\n",
      "Loss: 1.0981, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3400\n",
      "epoch: 12\n",
      "Loss: 1.0988, Accuracy: 0.3438\n",
      "Loss: 1.0988, Accuracy: 0.3750\n",
      "Loss: 1.0997, Accuracy: 0.3125\n",
      "Loss: 1.1011, Accuracy: 0.2500\n",
      "Loss: 1.1016, Accuracy: 0.1875\n",
      "Loss: 1.0981, Accuracy: 0.3750\n",
      "Loss: 1.0988, Accuracy: 0.3438\n",
      "Loss: 1.0995, Accuracy: 0.2812\n",
      "Loss: 1.0991, Accuracy: 0.2500\n",
      "Loss: 1.0965, Accuracy: 0.4688\n",
      "Loss: 1.0954, Accuracy: 0.4062\n",
      "Loss: 1.1030, Accuracy: 0.1875\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 13\n",
      "Loss: 1.0952, Accuracy: 0.4375\n",
      "Loss: 1.0989, Accuracy: 0.3750\n",
      "Loss: 1.0954, Accuracy: 0.3438\n",
      "Loss: 1.1045, Accuracy: 0.3438\n",
      "Loss: 1.1052, Accuracy: 0.2500\n",
      "Loss: 1.0944, Accuracy: 0.4375\n",
      "Loss: 1.0968, Accuracy: 0.3438\n",
      "Loss: 1.0971, Accuracy: 0.3125\n",
      "Loss: 1.0913, Accuracy: 0.4688\n",
      "Loss: 1.0966, Accuracy: 0.3125\n",
      "Loss: 1.0962, Accuracy: 0.4375\n",
      "Loss: 1.0945, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 14\n",
      "Loss: 1.1047, Accuracy: 0.3125\n",
      "Loss: 1.1157, Accuracy: 0.1562\n",
      "Loss: 1.0978, Accuracy: 0.3750\n",
      "Loss: 1.0987, Accuracy: 0.3438\n",
      "Loss: 1.0942, Accuracy: 0.4375\n",
      "Loss: 1.1006, Accuracy: 0.3125\n",
      "Loss: 1.0947, Accuracy: 0.4062\n",
      "Loss: 1.0902, Accuracy: 0.4688\n",
      "Loss: 1.0931, Accuracy: 0.4062\n",
      "Loss: 1.1141, Accuracy: 0.1875\n",
      "Loss: 1.0980, Accuracy: 0.3438\n",
      "Loss: 1.0994, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 15\n",
      "Loss: 1.0956, Accuracy: 0.3750\n",
      "Loss: 1.0998, Accuracy: 0.3750\n",
      "Loss: 1.1006, Accuracy: 0.3125\n",
      "Loss: 1.1004, Accuracy: 0.3125\n",
      "Loss: 1.1028, Accuracy: 0.2812\n",
      "Loss: 1.0984, Accuracy: 0.3438\n",
      "Loss: 1.0954, Accuracy: 0.4062\n",
      "Loss: 1.0975, Accuracy: 0.3750\n",
      "Loss: 1.1015, Accuracy: 0.2812\n",
      "Loss: 1.0963, Accuracy: 0.4062\n",
      "Loss: 1.1002, Accuracy: 0.3438\n",
      "Loss: 1.0906, Accuracy: 0.4688\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 16\n",
      "Loss: 1.1031, Accuracy: 0.2812\n",
      "Loss: 1.1110, Accuracy: 0.1875\n",
      "Loss: 1.0998, Accuracy: 0.3125\n",
      "Loss: 1.0923, Accuracy: 0.5000\n",
      "Loss: 1.0990, Accuracy: 0.3438\n",
      "Loss: 1.1025, Accuracy: 0.2812\n",
      "Loss: 1.1045, Accuracy: 0.2188\n",
      "Loss: 1.0958, Accuracy: 0.4062\n",
      "Loss: 1.1008, Accuracy: 0.2500\n",
      "Loss: 1.1068, Accuracy: 0.2188\n",
      "Loss: 1.0968, Accuracy: 0.4375\n",
      "Loss: 1.0987, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 17\n",
      "Loss: 1.1035, Accuracy: 0.2188\n",
      "Loss: 1.0942, Accuracy: 0.4688\n",
      "Loss: 1.1017, Accuracy: 0.2812\n",
      "Loss: 1.0946, Accuracy: 0.3750\n",
      "Loss: 1.0993, Accuracy: 0.2500\n",
      "Loss: 1.0956, Accuracy: 0.4688\n",
      "Loss: 1.0970, Accuracy: 0.3750\n",
      "Loss: 1.0959, Accuracy: 0.4062\n",
      "Loss: 1.0952, Accuracy: 0.3438\n",
      "Loss: 1.0909, Accuracy: 0.4375\n",
      "Loss: 1.1021, Accuracy: 0.2500\n",
      "Loss: 1.0985, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 18\n",
      "Loss: 1.1137, Accuracy: 0.2500\n",
      "Loss: 1.0989, Accuracy: 0.2812\n",
      "Loss: 1.0944, Accuracy: 0.3125\n",
      "Loss: 1.0970, Accuracy: 0.4375\n",
      "Loss: 1.0997, Accuracy: 0.4062\n",
      "Loss: 1.0991, Accuracy: 0.3438\n",
      "Loss: 1.0947, Accuracy: 0.3438\n",
      "Loss: 1.0921, Accuracy: 0.4062\n",
      "Loss: 1.0958, Accuracy: 0.3438\n",
      "Loss: 1.0922, Accuracy: 0.3750\n",
      "Loss: 1.1121, Accuracy: 0.2188\n",
      "Loss: 1.1039, Accuracy: 0.2500\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 19\n",
      "Loss: 1.0908, Accuracy: 0.4375\n",
      "Loss: 1.1131, Accuracy: 0.2812\n",
      "Loss: 1.0950, Accuracy: 0.4688\n",
      "Loss: 1.0965, Accuracy: 0.3750\n",
      "Loss: 1.1012, Accuracy: 0.2812\n",
      "Loss: 1.0994, Accuracy: 0.3750\n",
      "Loss: 1.1022, Accuracy: 0.3125\n",
      "Loss: 1.0915, Accuracy: 0.4688\n",
      "Loss: 1.0881, Accuracy: 0.4688\n",
      "Loss: 1.0958, Accuracy: 0.4062\n",
      "Loss: 1.0957, Accuracy: 0.3750\n",
      "Loss: 1.1051, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 20\n",
      "Loss: 1.0907, Accuracy: 0.4375\n",
      "Loss: 1.1021, Accuracy: 0.3125\n",
      "Loss: 1.0893, Accuracy: 0.4375\n",
      "Loss: 1.0859, Accuracy: 0.4688\n",
      "Loss: 1.1034, Accuracy: 0.3125\n",
      "Loss: 1.1030, Accuracy: 0.3125\n",
      "Loss: 1.0988, Accuracy: 0.3438\n",
      "Loss: 1.0896, Accuracy: 0.4375\n",
      "Loss: 1.1137, Accuracy: 0.2188\n",
      "Loss: 1.0993, Accuracy: 0.3438\n",
      "Loss: 1.1089, Accuracy: 0.2188\n",
      "Loss: 1.0913, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 21\n",
      "Loss: 1.1051, Accuracy: 0.2812\n",
      "Loss: 1.0961, Accuracy: 0.3750\n",
      "Loss: 1.1012, Accuracy: 0.3125\n",
      "Loss: 1.0953, Accuracy: 0.4062\n",
      "Loss: 1.1020, Accuracy: 0.3125\n",
      "Loss: 1.0988, Accuracy: 0.3438\n",
      "Loss: 1.1010, Accuracy: 0.3125\n",
      "Loss: 1.1019, Accuracy: 0.3125\n",
      "Loss: 1.0965, Accuracy: 0.3750\n",
      "Loss: 1.0962, Accuracy: 0.3750\n",
      "Loss: 1.1018, Accuracy: 0.2812\n",
      "Loss: 1.0948, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 22\n",
      "Loss: 1.0983, Accuracy: 0.3750\n",
      "Loss: 1.1037, Accuracy: 0.3125\n",
      "Loss: 1.0939, Accuracy: 0.3438\n",
      "Loss: 1.1064, Accuracy: 0.2500\n",
      "Loss: 1.1006, Accuracy: 0.2188\n",
      "Loss: 1.1002, Accuracy: 0.1875\n",
      "Loss: 1.1106, Accuracy: 0.2500\n",
      "Loss: 1.0947, Accuracy: 0.4375\n",
      "Loss: 1.0962, Accuracy: 0.3750\n",
      "Loss: 1.1005, Accuracy: 0.3438\n",
      "Loss: 1.1040, Accuracy: 0.2188\n",
      "Loss: 1.0968, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 23\n",
      "Loss: 1.1060, Accuracy: 0.2500\n",
      "Loss: 1.0993, Accuracy: 0.3125\n",
      "Loss: 1.0974, Accuracy: 0.3750\n",
      "Loss: 1.1020, Accuracy: 0.2812\n",
      "Loss: 1.0978, Accuracy: 0.3125\n",
      "Loss: 1.0946, Accuracy: 0.4688\n",
      "Loss: 1.0952, Accuracy: 0.3438\n",
      "Loss: 1.1072, Accuracy: 0.2812\n",
      "Loss: 1.0947, Accuracy: 0.4375\n",
      "Loss: 1.0955, Accuracy: 0.3438\n",
      "Loss: 1.1044, Accuracy: 0.2500\n",
      "Loss: 1.0960, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 24\n",
      "Loss: 1.0942, Accuracy: 0.4688\n",
      "Loss: 1.0942, Accuracy: 0.4062\n",
      "Loss: 1.0993, Accuracy: 0.3125\n",
      "Loss: 1.1050, Accuracy: 0.3125\n",
      "Loss: 1.1016, Accuracy: 0.2812\n",
      "Loss: 1.0979, Accuracy: 0.3750\n",
      "Loss: 1.0995, Accuracy: 0.3438\n",
      "Loss: 1.0927, Accuracy: 0.4375\n",
      "Loss: 1.0938, Accuracy: 0.4375\n",
      "Loss: 1.1033, Accuracy: 0.2812\n",
      "Loss: 1.1038, Accuracy: 0.2812\n",
      "Loss: 1.0984, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 25\n",
      "Loss: 1.0946, Accuracy: 0.4062\n",
      "Loss: 1.0989, Accuracy: 0.3750\n",
      "Loss: 1.0932, Accuracy: 0.4375\n",
      "Loss: 1.1009, Accuracy: 0.3125\n",
      "Loss: 1.1046, Accuracy: 0.3125\n",
      "Loss: 1.1061, Accuracy: 0.2812\n",
      "Loss: 1.0941, Accuracy: 0.4062\n",
      "Loss: 1.0981, Accuracy: 0.3438\n",
      "Loss: 1.0907, Accuracy: 0.4688\n",
      "Loss: 1.0950, Accuracy: 0.3750\n",
      "Loss: 1.1079, Accuracy: 0.2812\n",
      "Loss: 1.0884, Accuracy: 0.4688\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 26\n",
      "Loss: 1.1140, Accuracy: 0.1875\n",
      "Loss: 1.1078, Accuracy: 0.2500\n",
      "Loss: 1.1038, Accuracy: 0.2500\n",
      "Loss: 1.1012, Accuracy: 0.2812\n",
      "Loss: 1.0942, Accuracy: 0.4688\n",
      "Loss: 1.0963, Accuracy: 0.3750\n",
      "Loss: 1.0978, Accuracy: 0.3750\n",
      "Loss: 1.1022, Accuracy: 0.2812\n",
      "Loss: 1.1006, Accuracy: 0.3125\n",
      "Loss: 1.0951, Accuracy: 0.4062\n",
      "Loss: 1.0901, Accuracy: 0.5000\n",
      "Loss: 1.0833, Accuracy: 0.5312\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 27\n",
      "Loss: 1.0950, Accuracy: 0.3750\n",
      "Loss: 1.0930, Accuracy: 0.4062\n",
      "Loss: 1.0848, Accuracy: 0.4688\n",
      "Loss: 1.1034, Accuracy: 0.3125\n",
      "Loss: 1.1109, Accuracy: 0.2500\n",
      "Loss: 1.1033, Accuracy: 0.3125\n",
      "Loss: 1.1149, Accuracy: 0.1875\n",
      "Loss: 1.0978, Accuracy: 0.3438\n",
      "Loss: 1.1058, Accuracy: 0.2812\n",
      "Loss: 1.1045, Accuracy: 0.2500\n",
      "Loss: 1.0984, Accuracy: 0.3438\n",
      "Loss: 1.0966, Accuracy: 0.3750\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 28\n",
      "Loss: 1.1031, Accuracy: 0.2500\n",
      "Loss: 1.0908, Accuracy: 0.5000\n",
      "Loss: 1.0965, Accuracy: 0.3750\n",
      "Loss: 1.0996, Accuracy: 0.3125\n",
      "Loss: 1.1069, Accuracy: 0.2188\n",
      "Loss: 1.0954, Accuracy: 0.4375\n",
      "Loss: 1.0996, Accuracy: 0.3438\n",
      "Loss: 1.0966, Accuracy: 0.3750\n",
      "Loss: 1.0909, Accuracy: 0.4688\n",
      "Loss: 1.1093, Accuracy: 0.2188\n",
      "Loss: 1.0929, Accuracy: 0.4375\n",
      "Loss: 1.0869, Accuracy: 0.5000\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 29\n",
      "Loss: 1.1077, Accuracy: 0.2500\n",
      "Loss: 1.1075, Accuracy: 0.2500\n",
      "Loss: 1.0959, Accuracy: 0.3750\n",
      "Loss: 1.0876, Accuracy: 0.4688\n",
      "Loss: 1.1073, Accuracy: 0.2812\n",
      "Loss: 1.1069, Accuracy: 0.2188\n",
      "Loss: 1.0979, Accuracy: 0.3438\n",
      "Loss: 1.0987, Accuracy: 0.4062\n",
      "Loss: 1.0912, Accuracy: 0.4375\n",
      "Loss: 1.0883, Accuracy: 0.4375\n",
      "Loss: 1.0923, Accuracy: 0.4062\n",
      "Loss: 1.0903, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 30\n",
      "Loss: 1.0896, Accuracy: 0.4062\n",
      "Loss: 1.0966, Accuracy: 0.4375\n",
      "Loss: 1.0919, Accuracy: 0.4062\n",
      "Loss: 1.1045, Accuracy: 0.3125\n",
      "Loss: 1.0944, Accuracy: 0.3750\n",
      "Loss: 1.1032, Accuracy: 0.3125\n",
      "Loss: 1.1070, Accuracy: 0.2812\n",
      "Loss: 1.1047, Accuracy: 0.3125\n",
      "Loss: 1.0951, Accuracy: 0.3750\n",
      "Loss: 1.1038, Accuracy: 0.3125\n",
      "Loss: 1.0986, Accuracy: 0.3438\n",
      "Loss: 1.0897, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 31\n",
      "Loss: 1.0952, Accuracy: 0.3750\n",
      "Loss: 1.1183, Accuracy: 0.1562\n",
      "Loss: 1.1041, Accuracy: 0.2500\n",
      "Loss: 1.0978, Accuracy: 0.4062\n",
      "Loss: 1.0943, Accuracy: 0.4062\n",
      "Loss: 1.0982, Accuracy: 0.3438\n",
      "Loss: 1.0982, Accuracy: 0.3438\n",
      "Loss: 1.0931, Accuracy: 0.4062\n",
      "Loss: 1.0935, Accuracy: 0.4062\n",
      "Loss: 1.1001, Accuracy: 0.3125\n",
      "Loss: 1.0947, Accuracy: 0.3750\n",
      "Loss: 1.0852, Accuracy: 0.4688\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 32\n",
      "Loss: 1.1227, Accuracy: 0.0938\n",
      "Loss: 1.0894, Accuracy: 0.4062\n",
      "Loss: 1.0770, Accuracy: 0.5000\n",
      "Loss: 1.0884, Accuracy: 0.4688\n",
      "Loss: 1.1033, Accuracy: 0.3125\n",
      "Loss: 1.1163, Accuracy: 0.2812\n",
      "Loss: 1.0878, Accuracy: 0.3438\n",
      "Loss: 1.0948, Accuracy: 0.4688\n",
      "Loss: 1.0910, Accuracy: 0.4062\n",
      "Loss: 1.1159, Accuracy: 0.2500\n",
      "Loss: 1.1012, Accuracy: 0.2188\n",
      "Loss: 1.0819, Accuracy: 0.5312\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 33\n",
      "Loss: 1.1021, Accuracy: 0.3750\n",
      "Loss: 1.1052, Accuracy: 0.2812\n",
      "Loss: 1.0903, Accuracy: 0.4688\n",
      "Loss: 1.0882, Accuracy: 0.4375\n",
      "Loss: 1.1148, Accuracy: 0.2188\n",
      "Loss: 1.0942, Accuracy: 0.2812\n",
      "Loss: 1.1090, Accuracy: 0.3750\n",
      "Loss: 1.0980, Accuracy: 0.3438\n",
      "Loss: 1.0986, Accuracy: 0.4062\n",
      "Loss: 1.0863, Accuracy: 0.4062\n",
      "Loss: 1.1052, Accuracy: 0.2812\n",
      "Loss: 1.1107, Accuracy: 0.2188\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 34\n",
      "Loss: 1.0978, Accuracy: 0.3438\n",
      "Loss: 1.1026, Accuracy: 0.3438\n",
      "Loss: 1.1112, Accuracy: 0.1875\n",
      "Loss: 1.1099, Accuracy: 0.1562\n",
      "Loss: 1.0962, Accuracy: 0.2812\n",
      "Loss: 1.1064, Accuracy: 0.2188\n",
      "Loss: 1.1007, Accuracy: 0.3125\n",
      "Loss: 1.1003, Accuracy: 0.3125\n",
      "Loss: 1.0971, Accuracy: 0.3438\n",
      "Loss: 1.1046, Accuracy: 0.3125\n",
      "Loss: 1.0982, Accuracy: 0.3438\n",
      "Loss: 1.0988, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 35\n",
      "Loss: 1.0965, Accuracy: 0.4062\n",
      "Loss: 1.0972, Accuracy: 0.3750\n",
      "Loss: 1.0975, Accuracy: 0.3438\n",
      "Loss: 1.1016, Accuracy: 0.2500\n",
      "Loss: 1.1002, Accuracy: 0.2500\n",
      "Loss: 1.1036, Accuracy: 0.2188\n",
      "Loss: 1.0985, Accuracy: 0.4062\n",
      "Loss: 1.0939, Accuracy: 0.5312\n",
      "Loss: 1.1018, Accuracy: 0.2500\n",
      "Loss: 1.0969, Accuracy: 0.3438\n",
      "Loss: 1.1009, Accuracy: 0.3125\n",
      "Loss: 1.1042, Accuracy: 0.2188\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 36\n",
      "Loss: 1.0962, Accuracy: 0.5000\n",
      "Loss: 1.0987, Accuracy: 0.3438\n",
      "Loss: 1.1006, Accuracy: 0.2812\n",
      "Loss: 1.1039, Accuracy: 0.2500\n",
      "Loss: 1.1003, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0997, Accuracy: 0.3750\n",
      "Loss: 1.1002, Accuracy: 0.2500\n",
      "Loss: 1.0983, Accuracy: 0.3438\n",
      "Loss: 1.0986, Accuracy: 0.4062\n",
      "Loss: 1.0993, Accuracy: 0.3125\n",
      "Loss: 1.0989, Accuracy: 0.3125\n",
      "Loss: 1.0987, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3400\n",
      "epoch: 37\n",
      "Loss: 1.0995, Accuracy: 0.2188\n",
      "Loss: 1.0988, Accuracy: 0.3438\n",
      "Loss: 1.0986, Accuracy: 0.3438\n",
      "Loss: 1.1007, Accuracy: 0.2500\n",
      "Loss: 1.0987, Accuracy: 0.3125\n",
      "Loss: 1.0984, Accuracy: 0.5000\n",
      "Loss: 1.0993, Accuracy: 0.3125\n",
      "Loss: 1.0994, Accuracy: 0.3125\n",
      "Loss: 1.0996, Accuracy: 0.2812\n",
      "Loss: 1.0993, Accuracy: 0.3438\n",
      "Loss: 1.0988, Accuracy: 0.2812\n",
      "Loss: 1.0984, Accuracy: 0.3125\n",
      "Test Accuracy: 0.5000\n",
      "epoch: 38\n",
      "Loss: 1.0982, Accuracy: 0.5000\n",
      "Loss: 1.0996, Accuracy: 0.4375\n",
      "Loss: 1.0978, Accuracy: 0.3438\n",
      "Loss: 1.1006, Accuracy: 0.2188\n",
      "Loss: 1.0997, Accuracy: 0.2812\n",
      "Loss: 1.0990, Accuracy: 0.2812\n",
      "Loss: 1.0986, Accuracy: 0.3438\n",
      "Loss: 1.0983, Accuracy: 0.3750\n",
      "Loss: 1.0988, Accuracy: 0.3125\n",
      "Loss: 1.0974, Accuracy: 0.4062\n",
      "Loss: 1.1025, Accuracy: 0.2188\n",
      "Loss: 1.0990, Accuracy: 0.2500\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 39\n",
      "Loss: 1.0982, Accuracy: 0.5000\n",
      "Loss: 1.1006, Accuracy: 0.2500\n",
      "Loss: 1.0987, Accuracy: 0.2500\n",
      "Loss: 1.0962, Accuracy: 0.4375\n",
      "Loss: 1.0972, Accuracy: 0.3750\n",
      "Loss: 1.0936, Accuracy: 0.4375\n",
      "Loss: 1.0999, Accuracy: 0.2812\n",
      "Loss: 1.1004, Accuracy: 0.3438\n",
      "Loss: 1.0960, Accuracy: 0.3438\n",
      "Loss: 1.0968, Accuracy: 0.4375\n",
      "Loss: 1.1004, Accuracy: 0.3125\n",
      "Loss: 1.1061, Accuracy: 0.1250\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 40\n",
      "Loss: 1.0921, Accuracy: 0.4375\n",
      "Loss: 1.0997, Accuracy: 0.3438\n",
      "Loss: 1.0890, Accuracy: 0.3125\n",
      "Loss: 1.0974, Accuracy: 0.3750\n",
      "Loss: 1.1003, Accuracy: 0.3438\n",
      "Loss: 1.1057, Accuracy: 0.3125\n",
      "Loss: 1.0992, Accuracy: 0.3750\n",
      "Loss: 1.0930, Accuracy: 0.3125\n",
      "Loss: 1.0943, Accuracy: 0.4375\n",
      "Loss: 1.1163, Accuracy: 0.2812\n",
      "Loss: 1.0915, Accuracy: 0.5000\n",
      "Loss: 1.1202, Accuracy: 0.0938\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 41\n",
      "Loss: 1.0925, Accuracy: 0.5000\n",
      "Loss: 1.0993, Accuracy: 0.2812\n",
      "Loss: 1.0921, Accuracy: 0.5000\n",
      "Loss: 1.0966, Accuracy: 0.3438\n",
      "Loss: 1.1025, Accuracy: 0.3125\n",
      "Loss: 1.1021, Accuracy: 0.2812\n",
      "Loss: 1.0917, Accuracy: 0.4375\n",
      "Loss: 1.0995, Accuracy: 0.4062\n",
      "Loss: 1.1076, Accuracy: 0.2500\n",
      "Loss: 1.1012, Accuracy: 0.3125\n",
      "Loss: 1.1021, Accuracy: 0.2812\n",
      "Loss: 1.1060, Accuracy: 0.1875\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 42\n",
      "Loss: 1.0947, Accuracy: 0.4688\n",
      "Loss: 1.0969, Accuracy: 0.3438\n",
      "Loss: 1.0882, Accuracy: 0.5000\n",
      "Loss: 1.1053, Accuracy: 0.2812\n",
      "Loss: 1.1040, Accuracy: 0.2812\n",
      "Loss: 1.1010, Accuracy: 0.2812\n",
      "Loss: 1.0985, Accuracy: 0.2812\n",
      "Loss: 1.1042, Accuracy: 0.2500\n",
      "Loss: 1.1020, Accuracy: 0.3125\n",
      "Loss: 1.0966, Accuracy: 0.4688\n",
      "Loss: 1.1007, Accuracy: 0.2812\n",
      "Loss: 1.0998, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 43\n",
      "Loss: 1.0978, Accuracy: 0.3750\n",
      "Loss: 1.1014, Accuracy: 0.2188\n",
      "Loss: 1.0977, Accuracy: 0.4688\n",
      "Loss: 1.0980, Accuracy: 0.3125\n",
      "Loss: 1.1027, Accuracy: 0.2812\n",
      "Loss: 1.0985, Accuracy: 0.2812\n",
      "Loss: 1.0996, Accuracy: 0.3125\n",
      "Loss: 1.0976, Accuracy: 0.3750\n",
      "Loss: 1.1005, Accuracy: 0.3438\n",
      "Loss: 1.0971, Accuracy: 0.4375\n",
      "Loss: 1.0988, Accuracy: 0.3438\n",
      "Loss: 1.0987, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 44\n",
      "Loss: 1.0974, Accuracy: 0.3438\n",
      "Loss: 1.0966, Accuracy: 0.4062\n",
      "Loss: 1.0969, Accuracy: 0.3750\n",
      "Loss: 1.1014, Accuracy: 0.3125\n",
      "Loss: 1.1001, Accuracy: 0.3125\n",
      "Loss: 1.0996, Accuracy: 0.3125\n",
      "Loss: 1.1006, Accuracy: 0.2812\n",
      "Loss: 1.1000, Accuracy: 0.2812\n",
      "Loss: 1.0999, Accuracy: 0.2500\n",
      "Loss: 1.0987, Accuracy: 0.3438\n",
      "Loss: 1.0973, Accuracy: 0.4062\n",
      "Loss: 1.1008, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 45\n",
      "Loss: 1.1006, Accuracy: 0.2188\n",
      "Loss: 1.0987, Accuracy: 0.2188\n",
      "Loss: 1.0967, Accuracy: 0.4375\n",
      "Loss: 1.0961, Accuracy: 0.4062\n",
      "Loss: 1.1102, Accuracy: 0.1250\n",
      "Loss: 1.0978, Accuracy: 0.4062\n",
      "Loss: 1.1014, Accuracy: 0.1875\n",
      "Loss: 1.1006, Accuracy: 0.2188\n",
      "Loss: 1.0989, Accuracy: 0.2188\n",
      "Loss: 1.0973, Accuracy: 0.4375\n",
      "Loss: 1.0981, Accuracy: 0.3438\n",
      "Loss: 1.0990, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 46\n",
      "Loss: 1.0923, Accuracy: 0.5625\n",
      "Loss: 1.1016, Accuracy: 0.1875\n",
      "Loss: 1.0935, Accuracy: 0.4375\n",
      "Loss: 1.1052, Accuracy: 0.2500\n",
      "Loss: 1.0921, Accuracy: 0.3750\n",
      "Loss: 1.0932, Accuracy: 0.1875\n",
      "Loss: 1.1032, Accuracy: 0.1875\n",
      "Loss: 1.1030, Accuracy: 0.2812\n",
      "Loss: 1.0951, Accuracy: 0.3750\n",
      "Loss: 1.0896, Accuracy: 0.4375\n",
      "Loss: 1.1129, Accuracy: 0.2500\n",
      "Loss: 1.0979, Accuracy: 0.5000\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 47\n",
      "Loss: 1.0963, Accuracy: 0.3438\n",
      "Loss: 1.0871, Accuracy: 0.4375\n",
      "Loss: 1.1020, Accuracy: 0.2500\n",
      "Loss: 1.1027, Accuracy: 0.3438\n",
      "Loss: 1.0954, Accuracy: 0.3438\n",
      "Loss: 1.0984, Accuracy: 0.3125\n",
      "Loss: 1.1000, Accuracy: 0.3438\n",
      "Loss: 1.1006, Accuracy: 0.3125\n",
      "Loss: 1.0936, Accuracy: 0.4375\n",
      "Loss: 1.1062, Accuracy: 0.2812\n",
      "Loss: 1.0922, Accuracy: 0.4062\n",
      "Loss: 1.0841, Accuracy: 0.5000\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 48\n",
      "Loss: 1.1001, Accuracy: 0.3438\n",
      "Loss: 1.1066, Accuracy: 0.2188\n",
      "Loss: 1.1009, Accuracy: 0.3125\n",
      "Loss: 1.0976, Accuracy: 0.3438\n",
      "Loss: 1.1030, Accuracy: 0.3125\n",
      "Loss: 1.0933, Accuracy: 0.4375\n",
      "Loss: 1.0968, Accuracy: 0.3125\n",
      "Loss: 1.0961, Accuracy: 0.3125\n",
      "Loss: 1.1012, Accuracy: 0.4375\n",
      "Loss: 1.1082, Accuracy: 0.2812\n",
      "Loss: 1.0898, Accuracy: 0.4688\n",
      "Loss: 1.1048, Accuracy: 0.3750\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 49\n",
      "Loss: 1.1056, Accuracy: 0.2500\n",
      "Loss: 1.0954, Accuracy: 0.3438\n",
      "Loss: 1.0963, Accuracy: 0.3438\n",
      "Loss: 1.0907, Accuracy: 0.3438\n",
      "Loss: 1.0967, Accuracy: 0.3750\n",
      "Loss: 1.1085, Accuracy: 0.3125\n",
      "Loss: 1.0981, Accuracy: 0.2812\n",
      "Loss: 1.0944, Accuracy: 0.5000\n",
      "Loss: 1.0894, Accuracy: 0.4375\n",
      "Loss: 1.1016, Accuracy: 0.3750\n",
      "Loss: 1.0863, Accuracy: 0.5000\n",
      "Loss: 1.1049, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 50\n",
      "Loss: 1.0818, Accuracy: 0.4688\n",
      "Loss: 1.1140, Accuracy: 0.2500\n",
      "Loss: 1.1164, Accuracy: 0.1875\n",
      "Loss: 1.1008, Accuracy: 0.2812\n",
      "Loss: 1.1064, Accuracy: 0.2500\n",
      "Loss: 1.0965, Accuracy: 0.3438\n",
      "Loss: 1.0943, Accuracy: 0.4062\n",
      "Loss: 1.1016, Accuracy: 0.2812\n",
      "Loss: 1.0969, Accuracy: 0.3125\n",
      "Loss: 1.0924, Accuracy: 0.5312\n",
      "Loss: 1.0945, Accuracy: 0.4062\n",
      "Loss: 1.1041, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 51\n",
      "Loss: 1.0949, Accuracy: 0.3438\n",
      "Loss: 1.0880, Accuracy: 0.4688\n",
      "Loss: 1.0859, Accuracy: 0.4688\n",
      "Loss: 1.1085, Accuracy: 0.3438\n",
      "Loss: 1.1071, Accuracy: 0.2812\n",
      "Loss: 1.1076, Accuracy: 0.2500\n",
      "Loss: 1.1031, Accuracy: 0.2812\n",
      "Loss: 1.1019, Accuracy: 0.2812\n",
      "Loss: 1.0893, Accuracy: 0.4688\n",
      "Loss: 1.0881, Accuracy: 0.5000\n",
      "Loss: 1.1088, Accuracy: 0.2500\n",
      "Loss: 1.0932, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 52\n",
      "Loss: 1.0944, Accuracy: 0.3750\n",
      "Loss: 1.0992, Accuracy: 0.3750\n",
      "Loss: 1.0893, Accuracy: 0.4375\n",
      "Loss: 1.0960, Accuracy: 0.3750\n",
      "Loss: 1.0944, Accuracy: 0.3750\n",
      "Loss: 1.1071, Accuracy: 0.2812\n",
      "Loss: 1.0886, Accuracy: 0.4375\n",
      "Loss: 1.0884, Accuracy: 0.4375\n",
      "Loss: 1.1150, Accuracy: 0.2188\n",
      "Loss: 1.1110, Accuracy: 0.2188\n",
      "Loss: 1.1012, Accuracy: 0.3125\n",
      "Loss: 1.0991, Accuracy: 0.3438\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 53\n",
      "Loss: 1.0910, Accuracy: 0.3750\n",
      "Loss: 1.0884, Accuracy: 0.4062\n",
      "Loss: 1.0862, Accuracy: 0.4688\n",
      "Loss: 1.0865, Accuracy: 0.4375\n",
      "Loss: 1.1029, Accuracy: 0.2500\n",
      "Loss: 1.0789, Accuracy: 0.4062\n",
      "Loss: 1.0909, Accuracy: 0.4062\n",
      "Loss: 1.1223, Accuracy: 0.2500\n",
      "Loss: 1.1006, Accuracy: 0.4062\n",
      "Loss: 1.1189, Accuracy: 0.2188\n",
      "Loss: 1.0904, Accuracy: 0.4688\n",
      "Loss: 1.1013, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 54\n",
      "Loss: 1.1016, Accuracy: 0.3125\n",
      "Loss: 1.1127, Accuracy: 0.2500\n",
      "Loss: 1.1077, Accuracy: 0.2500\n",
      "Loss: 1.1080, Accuracy: 0.1875\n",
      "Loss: 1.0949, Accuracy: 0.4375\n",
      "Loss: 1.0953, Accuracy: 0.3750\n",
      "Loss: 1.0900, Accuracy: 0.4688\n",
      "Loss: 1.0887, Accuracy: 0.4688\n",
      "Loss: 1.1128, Accuracy: 0.2188\n",
      "Loss: 1.1037, Accuracy: 0.2812\n",
      "Loss: 1.1024, Accuracy: 0.2812\n",
      "Loss: 1.1009, Accuracy: 0.2812\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 55\n",
      "Loss: 1.1002, Accuracy: 0.3125\n",
      "Loss: 1.0980, Accuracy: 0.3438\n",
      "Loss: 1.0994, Accuracy: 0.2812\n",
      "Loss: 1.0993, Accuracy: 0.2812\n",
      "Loss: 1.0984, Accuracy: 0.3750\n",
      "Loss: 1.0988, Accuracy: 0.3125\n",
      "Loss: 1.1000, Accuracy: 0.3750\n",
      "Loss: 1.0946, Accuracy: 0.4688\n",
      "Loss: 1.1000, Accuracy: 0.2812\n",
      "Loss: 1.1028, Accuracy: 0.1562\n",
      "Loss: 1.0915, Accuracy: 0.4375\n",
      "Loss: 1.0997, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3600\n",
      "epoch: 56\n",
      "Loss: 1.0986, Accuracy: 0.3438\n",
      "Loss: 1.0925, Accuracy: 0.3438\n",
      "Loss: 1.1026, Accuracy: 0.1875\n",
      "Loss: 1.1059, Accuracy: 0.3125\n",
      "Loss: 1.0927, Accuracy: 0.4688\n",
      "Loss: 1.0942, Accuracy: 0.4375\n",
      "Loss: 1.0983, Accuracy: 0.3750\n",
      "Loss: 1.0914, Accuracy: 0.4062\n",
      "Loss: 1.0981, Accuracy: 0.3125\n",
      "Loss: 1.0989, Accuracy: 0.3125\n",
      "Loss: 1.0902, Accuracy: 0.4688\n",
      "Loss: 1.0970, Accuracy: 0.3750\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 57\n",
      "Loss: 1.1118, Accuracy: 0.2188\n",
      "Loss: 1.0862, Accuracy: 0.4375\n",
      "Loss: 1.0961, Accuracy: 0.4062\n",
      "Loss: 1.0881, Accuracy: 0.4375\n",
      "Loss: 1.0851, Accuracy: 0.4375\n",
      "Loss: 1.0993, Accuracy: 0.3750\n",
      "Loss: 1.1039, Accuracy: 0.3125\n",
      "Loss: 1.0894, Accuracy: 0.4062\n",
      "Loss: 1.1142, Accuracy: 0.2188\n",
      "Loss: 1.0980, Accuracy: 0.3750\n",
      "Loss: 1.1067, Accuracy: 0.2500\n",
      "Loss: 1.1005, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 58\n",
      "Loss: 1.1051, Accuracy: 0.2500\n",
      "Loss: 1.0902, Accuracy: 0.5000\n",
      "Loss: 1.0922, Accuracy: 0.4062\n",
      "Loss: 1.0967, Accuracy: 0.3438\n",
      "Loss: 1.0941, Accuracy: 0.3750\n",
      "Loss: 1.0914, Accuracy: 0.4375\n",
      "Loss: 1.1053, Accuracy: 0.2812\n",
      "Loss: 1.0975, Accuracy: 0.3438\n",
      "Loss: 1.0917, Accuracy: 0.4062\n",
      "Loss: 1.0990, Accuracy: 0.3438\n",
      "Loss: 1.0884, Accuracy: 0.4375\n",
      "Loss: 1.0904, Accuracy: 0.4062\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 59\n",
      "Loss: 1.1230, Accuracy: 0.1562\n",
      "Loss: 1.1032, Accuracy: 0.2812\n",
      "Loss: 1.1063, Accuracy: 0.2188\n",
      "Loss: 1.1046, Accuracy: 0.2812\n",
      "Loss: 1.1073, Accuracy: 0.1250\n",
      "Loss: 1.0987, Accuracy: 0.3438\n",
      "Loss: 1.1011, Accuracy: 0.1875\n",
      "Loss: 1.0967, Accuracy: 0.4688\n",
      "Loss: 1.1005, Accuracy: 0.1875\n",
      "Loss: 1.1006, Accuracy: 0.2188\n",
      "Loss: 1.0974, Accuracy: 0.4062\n",
      "Loss: 1.0968, Accuracy: 0.3125\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 60\n",
      "Loss: 1.0975, Accuracy: 0.4062\n",
      "Loss: 1.1021, Accuracy: 0.1875\n",
      "Loss: 1.0969, Accuracy: 0.3750\n",
      "Loss: 1.0972, Accuracy: 0.3438\n",
      "Loss: 1.1017, Accuracy: 0.1562\n",
      "Loss: 1.0968, Accuracy: 0.3438\n",
      "Loss: 1.1006, Accuracy: 0.2188\n",
      "Loss: 1.1002, Accuracy: 0.2188\n",
      "Loss: 1.0963, Accuracy: 0.3438\n",
      "Loss: 1.0992, Accuracy: 0.3438\n",
      "Loss: 1.0991, Accuracy: 0.1875\n",
      "Loss: 1.0949, Accuracy: 0.4375\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 61\n",
      "Loss: 1.0958, Accuracy: 0.3750\n",
      "Loss: 1.0965, Accuracy: 0.3438\n",
      "Loss: 1.0992, Accuracy: 0.2812\n",
      "Loss: 1.0940, Accuracy: 0.4375\n",
      "Loss: 1.0962, Accuracy: 0.3438\n",
      "Loss: 1.1007, Accuracy: 0.2500\n",
      "Loss: 1.0925, Accuracy: 0.5000\n",
      "Loss: 1.0930, Accuracy: 0.4062\n",
      "Loss: 1.0927, Accuracy: 0.4062\n",
      "Loss: 1.0844, Accuracy: 0.5000\n",
      "Loss: 1.0962, Accuracy: 0.3438\n",
      "Loss: 1.1077, Accuracy: 0.2188\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 62\n",
      "Loss: 1.1041, Accuracy: 0.2188\n",
      "Loss: 1.1038, Accuracy: 0.2812\n",
      "Loss: 1.0957, Accuracy: 0.3438\n",
      "Loss: 1.0970, Accuracy: 0.3125\n",
      "Loss: 1.0943, Accuracy: 0.3750\n",
      "Loss: 1.0930, Accuracy: 0.4062\n",
      "Loss: 1.0919, Accuracy: 0.3750\n",
      "Loss: 1.0882, Accuracy: 0.3750\n",
      "Loss: 1.0989, Accuracy: 0.2812\n",
      "Loss: 1.0905, Accuracy: 0.3438\n",
      "Loss: 1.0912, Accuracy: 0.5000\n",
      "Loss: 1.0885, Accuracy: 0.3750\n",
      "Test Accuracy: 0.3000\n",
      "epoch: 63\n",
      "Loss: 1.0900, Accuracy: 0.3750\n",
      "Loss: 1.1036, Accuracy: 0.2812\n",
      "Loss: 1.0972, Accuracy: 0.3125\n",
      "Loss: 1.0911, Accuracy: 0.4375\n",
      "Loss: 1.0937, Accuracy: 0.4375\n",
      "Loss: 1.1037, Accuracy: 0.2812\n",
      "Loss: 1.0993, Accuracy: 0.3125\n",
      "Loss: 1.0896, Accuracy: 0.3438\n",
      "Loss: 1.0975, Accuracy: 0.2500\n",
      "Loss: 1.0892, Accuracy: 0.7188\n",
      "Loss: 1.0947, Accuracy: 0.6250\n",
      "Loss: 1.0960, Accuracy: 0.6562\n",
      "Test Accuracy: 0.3950\n",
      "epoch: 64\n",
      "Loss: 1.0882, Accuracy: 0.4062\n",
      "Loss: 1.0951, Accuracy: 0.5000\n",
      "Loss: 1.0990, Accuracy: 0.3125\n",
      "Loss: 1.0974, Accuracy: 0.3125\n",
      "Loss: 1.0908, Accuracy: 0.5625\n",
      "Loss: 1.0911, Accuracy: 0.4375\n",
      "Loss: 1.0892, Accuracy: 0.4062\n",
      "Loss: 1.0962, Accuracy: 0.4062\n",
      "Loss: 1.0904, Accuracy: 0.4688\n",
      "Loss: 1.0905, Accuracy: 0.5625\n",
      "Loss: 1.0886, Accuracy: 0.6875\n",
      "Loss: 1.0903, Accuracy: 0.4688\n",
      "Test Accuracy: 0.6250\n",
      "epoch: 65\n",
      "Loss: 1.0900, Accuracy: 0.5625\n",
      "Loss: 1.0888, Accuracy: 0.5938\n",
      "Loss: 1.0861, Accuracy: 0.4062\n",
      "Loss: 1.0876, Accuracy: 0.4375\n",
      "Loss: 1.0851, Accuracy: 0.4688\n",
      "Loss: 1.0857, Accuracy: 0.4062\n",
      "Loss: 1.0881, Accuracy: 0.4688\n",
      "Loss: 1.0824, Accuracy: 0.5938\n",
      "Loss: 1.0847, Accuracy: 0.6250\n",
      "Loss: 1.0843, Accuracy: 0.5312\n",
      "Loss: 1.0809, Accuracy: 0.5000\n",
      "Loss: 1.0883, Accuracy: 0.3750\n",
      "Test Accuracy: 0.5400\n",
      "epoch: 66\n",
      "Loss: 1.0813, Accuracy: 0.5312\n",
      "Loss: 1.0770, Accuracy: 0.5625\n",
      "Loss: 1.0744, Accuracy: 0.6562\n",
      "Loss: 1.0719, Accuracy: 0.5625\n",
      "Loss: 1.0765, Accuracy: 0.7500\n",
      "Loss: 1.0712, Accuracy: 0.6562\n",
      "Loss: 1.0638, Accuracy: 0.8125\n",
      "Loss: 1.0695, Accuracy: 0.6875\n",
      "Loss: 1.0580, Accuracy: 0.6875\n",
      "Loss: 1.0440, Accuracy: 0.7812\n",
      "Loss: 1.0616, Accuracy: 0.6562\n",
      "Loss: 1.0489, Accuracy: 0.7188\n",
      "Test Accuracy: 0.6050\n",
      "epoch: 67\n",
      "Loss: 1.0627, Accuracy: 0.5312\n",
      "Loss: 1.0638, Accuracy: 0.5625\n",
      "Loss: 1.0322, Accuracy: 0.7500\n",
      "Loss: 1.0319, Accuracy: 0.6562\n",
      "Loss: 1.0428, Accuracy: 0.5938\n",
      "Loss: 1.0247, Accuracy: 0.6562\n",
      "Loss: 1.0491, Accuracy: 0.5312\n",
      "Loss: 1.0011, Accuracy: 0.7188\n",
      "Loss: 1.0007, Accuracy: 0.6562\n",
      "Loss: 0.9995, Accuracy: 0.7188\n",
      "Loss: 1.0140, Accuracy: 0.5938\n",
      "Loss: 0.9526, Accuracy: 0.8125\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 68\n",
      "Loss: 0.9873, Accuracy: 0.5938\n",
      "Loss: 0.9290, Accuracy: 0.7500\n",
      "Loss: 0.9562, Accuracy: 0.6875\n",
      "Loss: 0.8925, Accuracy: 0.7812\n",
      "Loss: 1.0306, Accuracy: 0.5625\n",
      "Loss: 0.9926, Accuracy: 0.6875\n",
      "Loss: 0.8860, Accuracy: 0.9062\n",
      "Loss: 0.8862, Accuracy: 0.6875\n",
      "Loss: 0.9033, Accuracy: 0.6562\n",
      "Loss: 0.7940, Accuracy: 0.8438\n",
      "Loss: 0.8149, Accuracy: 0.7812\n",
      "Loss: 0.8355, Accuracy: 0.7500\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 69\n",
      "Loss: 0.7581, Accuracy: 0.7188\n",
      "Loss: 0.7790, Accuracy: 0.7188\n",
      "Loss: 0.7678, Accuracy: 0.8125\n",
      "Loss: 0.7374, Accuracy: 0.7500\n",
      "Loss: 0.6341, Accuracy: 0.7812\n",
      "Loss: 0.7783, Accuracy: 0.6875\n",
      "Loss: 0.7817, Accuracy: 0.6250\n",
      "Loss: 0.6616, Accuracy: 0.7812\n",
      "Loss: 0.8524, Accuracy: 0.6875\n",
      "Loss: 0.8098, Accuracy: 0.5938\n",
      "Loss: 0.8134, Accuracy: 0.6562\n",
      "Loss: 0.8361, Accuracy: 0.6562\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 70\n",
      "Loss: 0.7186, Accuracy: 0.7500\n",
      "Loss: 0.7859, Accuracy: 0.6250\n",
      "Loss: 0.7395, Accuracy: 0.7188\n",
      "Loss: 0.6397, Accuracy: 0.6875\n",
      "Loss: 0.7359, Accuracy: 0.6250\n",
      "Loss: 0.7833, Accuracy: 0.6250\n",
      "Loss: 0.7109, Accuracy: 0.6562\n",
      "Loss: 0.6472, Accuracy: 0.5938\n",
      "Loss: 0.6365, Accuracy: 0.8125\n",
      "Loss: 0.7050, Accuracy: 0.6875\n",
      "Loss: 0.6538, Accuracy: 0.7188\n",
      "Loss: 0.7243, Accuracy: 0.7188\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 71\n",
      "Loss: 0.6763, Accuracy: 0.6562\n",
      "Loss: 0.6844, Accuracy: 0.7188\n",
      "Loss: 0.6852, Accuracy: 0.5938\n",
      "Loss: 0.7557, Accuracy: 0.5312\n",
      "Loss: 0.5987, Accuracy: 0.7812\n",
      "Loss: 0.6318, Accuracy: 0.6250\n",
      "Loss: 0.6975, Accuracy: 0.6875\n",
      "Loss: 0.6417, Accuracy: 0.6562\n",
      "Loss: 0.5625, Accuracy: 0.7812\n",
      "Loss: 0.6108, Accuracy: 0.6250\n",
      "Loss: 0.5824, Accuracy: 0.6875\n",
      "Loss: 0.7265, Accuracy: 0.5625\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 72\n",
      "Loss: 0.7062, Accuracy: 0.5625\n",
      "Loss: 0.6408, Accuracy: 0.5625\n",
      "Loss: 0.6599, Accuracy: 0.7500\n",
      "Loss: 0.6448, Accuracy: 0.6250\n",
      "Loss: 0.6295, Accuracy: 0.7500\n",
      "Loss: 0.7459, Accuracy: 0.5938\n",
      "Loss: 0.6534, Accuracy: 0.6875\n",
      "Loss: 0.5426, Accuracy: 0.8125\n",
      "Loss: 0.6312, Accuracy: 0.6562\n",
      "Loss: 0.5882, Accuracy: 0.7188\n",
      "Loss: 0.6627, Accuracy: 0.6875\n",
      "Loss: 0.6510, Accuracy: 0.7188\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 73\n",
      "Loss: 0.7211, Accuracy: 0.6250\n",
      "Loss: 0.5784, Accuracy: 0.6875\n",
      "Loss: 0.6571, Accuracy: 0.5625\n",
      "Loss: 0.5549, Accuracy: 0.7812\n",
      "Loss: 0.4959, Accuracy: 0.8750\n",
      "Loss: 0.6768, Accuracy: 0.6875\n",
      "Loss: 0.6354, Accuracy: 0.7500\n",
      "Loss: 0.6779, Accuracy: 0.6562\n",
      "Loss: 0.6718, Accuracy: 0.7812\n",
      "Loss: 0.5974, Accuracy: 0.6562\n",
      "Loss: 0.6029, Accuracy: 0.6250\n",
      "Loss: 0.7203, Accuracy: 0.6250\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 74\n",
      "Loss: 0.6479, Accuracy: 0.5938\n",
      "Loss: 0.8197, Accuracy: 0.5312\n",
      "Loss: 0.5394, Accuracy: 0.7188\n",
      "Loss: 0.5356, Accuracy: 0.8125\n",
      "Loss: 0.5969, Accuracy: 0.6562\n",
      "Loss: 0.5372, Accuracy: 0.7812\n",
      "Loss: 0.6717, Accuracy: 0.6562\n",
      "Loss: 0.7144, Accuracy: 0.5625\n",
      "Loss: 0.6344, Accuracy: 0.6562\n",
      "Loss: 0.5108, Accuracy: 0.7188\n",
      "Loss: 0.5758, Accuracy: 0.6875\n",
      "Loss: 0.7239, Accuracy: 0.5312\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 75\n",
      "Loss: 0.6377, Accuracy: 0.7188\n",
      "Loss: 0.5386, Accuracy: 0.7500\n",
      "Loss: 0.6084, Accuracy: 0.6562\n",
      "Loss: 0.5000, Accuracy: 0.7500\n",
      "Loss: 0.5987, Accuracy: 0.7188\n",
      "Loss: 0.6298, Accuracy: 0.7500\n",
      "Loss: 0.5623, Accuracy: 0.7188\n",
      "Loss: 0.4776, Accuracy: 0.7812\n",
      "Loss: 0.5996, Accuracy: 0.7188\n",
      "Loss: 0.5387, Accuracy: 0.6250\n",
      "Loss: 0.5579, Accuracy: 0.6562\n",
      "Loss: 0.4595, Accuracy: 0.7812\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 76\n",
      "Loss: 0.5320, Accuracy: 0.7812\n",
      "Loss: 0.5216, Accuracy: 0.6875\n",
      "Loss: 0.5629, Accuracy: 0.7188\n",
      "Loss: 0.6257, Accuracy: 0.6875\n",
      "Loss: 0.5510, Accuracy: 0.6562\n",
      "Loss: 0.6784, Accuracy: 0.5625\n",
      "Loss: 0.6192, Accuracy: 0.6875\n",
      "Loss: 0.6571, Accuracy: 0.6250\n",
      "Loss: 0.5378, Accuracy: 0.7188\n",
      "Loss: 0.5117, Accuracy: 0.8125\n",
      "Loss: 0.5146, Accuracy: 0.6562\n",
      "Loss: 0.4726, Accuracy: 0.7812\n",
      "Test Accuracy: 0.6400\n",
      "epoch: 77\n",
      "Loss: 0.5649, Accuracy: 0.5938\n",
      "Loss: 0.6154, Accuracy: 0.5312\n",
      "Loss: 0.4378, Accuracy: 0.7500\n",
      "Loss: 0.6084, Accuracy: 0.6250\n",
      "Loss: 0.6869, Accuracy: 0.5312\n",
      "Loss: 0.5802, Accuracy: 0.6562\n",
      "Loss: 0.5877, Accuracy: 0.6250\n",
      "Loss: 0.6065, Accuracy: 0.6250\n",
      "Loss: 0.5438, Accuracy: 0.6562\n",
      "Loss: 0.5557, Accuracy: 0.6250\n",
      "Loss: 0.5600, Accuracy: 0.6562\n",
      "Loss: 0.6075, Accuracy: 0.6250\n",
      "Test Accuracy: 0.6450\n",
      "epoch: 78\n",
      "Loss: 0.5026, Accuracy: 0.6562\n",
      "Loss: 0.5422, Accuracy: 0.5625\n",
      "Loss: 0.6361, Accuracy: 0.6875\n",
      "Loss: 0.5109, Accuracy: 0.6562\n",
      "Loss: 0.5198, Accuracy: 0.7812\n",
      "Loss: 0.5069, Accuracy: 0.7188\n",
      "Loss: 0.6299, Accuracy: 0.6562\n",
      "Loss: 0.5721, Accuracy: 0.7188\n",
      "Loss: 0.4940, Accuracy: 0.6875\n",
      "Loss: 0.6763, Accuracy: 0.6875\n",
      "Loss: 0.5183, Accuracy: 0.7188\n",
      "Loss: 0.4745, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6450\n",
      "epoch: 79\n",
      "Loss: 0.4385, Accuracy: 0.7812\n",
      "Loss: 0.4501, Accuracy: 0.8125\n",
      "Loss: 0.5192, Accuracy: 0.6875\n",
      "Loss: 0.4544, Accuracy: 0.8125\n",
      "Loss: 0.4867, Accuracy: 0.7500\n",
      "Loss: 0.4777, Accuracy: 0.7188\n",
      "Loss: 0.6062, Accuracy: 0.5312\n",
      "Loss: 0.6447, Accuracy: 0.5938\n",
      "Loss: 0.5745, Accuracy: 0.6875\n",
      "Loss: 0.6093, Accuracy: 0.5625\n",
      "Loss: 0.5380, Accuracy: 0.7500\n",
      "Loss: 0.6381, Accuracy: 0.5938\n",
      "Test Accuracy: 0.6500\n",
      "epoch: 80\n",
      "Loss: 0.5407, Accuracy: 0.7188\n",
      "Loss: 0.5301, Accuracy: 0.7500\n",
      "Loss: 0.5216, Accuracy: 0.7188\n",
      "Loss: 0.6050, Accuracy: 0.5938\n",
      "Loss: 0.5568, Accuracy: 0.6875\n",
      "Loss: 0.5304, Accuracy: 0.7188\n",
      "Loss: 0.5917, Accuracy: 0.6250\n",
      "Loss: 0.4729, Accuracy: 0.6250\n",
      "Loss: 0.4725, Accuracy: 0.7500\n",
      "Loss: 0.4669, Accuracy: 0.6562\n",
      "Loss: 0.5360, Accuracy: 0.8438\n",
      "Loss: 0.6467, Accuracy: 0.5000\n",
      "Test Accuracy: 0.6600\n",
      "epoch: 81\n",
      "Loss: 0.4948, Accuracy: 0.7500\n",
      "Loss: 0.5619, Accuracy: 0.5625\n",
      "Loss: 0.5774, Accuracy: 0.7188\n",
      "Loss: 0.6437, Accuracy: 0.5625\n",
      "Loss: 0.4701, Accuracy: 0.6562\n",
      "Loss: 0.4805, Accuracy: 0.7812\n",
      "Loss: 0.4729, Accuracy: 0.6562\n",
      "Loss: 0.5615, Accuracy: 0.6562\n",
      "Loss: 0.6487, Accuracy: 0.5938\n",
      "Loss: 0.5244, Accuracy: 0.7812\n",
      "Loss: 0.5252, Accuracy: 0.7500\n",
      "Loss: 0.5816, Accuracy: 0.5938\n",
      "Test Accuracy: 0.6500\n",
      "epoch: 82\n",
      "Loss: 0.4892, Accuracy: 0.5938\n",
      "Loss: 0.4772, Accuracy: 0.7188\n",
      "Loss: 0.6308, Accuracy: 0.5000\n",
      "Loss: 0.6243, Accuracy: 0.5625\n",
      "Loss: 0.5316, Accuracy: 0.7188\n",
      "Loss: 0.5901, Accuracy: 0.5312\n",
      "Loss: 0.5716, Accuracy: 0.6875\n",
      "Loss: 0.5830, Accuracy: 0.5938\n",
      "Loss: 0.5995, Accuracy: 0.6875\n",
      "Loss: 0.4894, Accuracy: 0.7188\n",
      "Loss: 0.4377, Accuracy: 0.6562\n",
      "Loss: 0.6274, Accuracy: 0.5938\n",
      "Test Accuracy: 0.6700\n",
      "epoch: 83\n",
      "Loss: 0.5150, Accuracy: 0.5938\n",
      "Loss: 0.5262, Accuracy: 0.7500\n",
      "Loss: 0.5394, Accuracy: 0.7188\n",
      "Loss: 0.5302, Accuracy: 0.7188\n",
      "Loss: 0.5501, Accuracy: 0.6875\n",
      "Loss: 0.4364, Accuracy: 0.7188\n",
      "Loss: 0.5026, Accuracy: 0.8125\n",
      "Loss: 0.6472, Accuracy: 0.5938\n",
      "Loss: 0.5788, Accuracy: 0.5938\n",
      "Loss: 0.5047, Accuracy: 0.7188\n",
      "Loss: 0.5866, Accuracy: 0.6250\n",
      "Loss: 0.6078, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6600\n",
      "epoch: 84\n",
      "Loss: 0.6285, Accuracy: 0.6875\n",
      "Loss: 0.5437, Accuracy: 0.6875\n",
      "Loss: 0.3576, Accuracy: 0.8125\n",
      "Loss: 0.5985, Accuracy: 0.5625\n",
      "Loss: 0.5355, Accuracy: 0.6875\n",
      "Loss: 0.5037, Accuracy: 0.8125\n",
      "Loss: 0.5997, Accuracy: 0.7188\n",
      "Loss: 0.6294, Accuracy: 0.5000\n",
      "Loss: 0.5461, Accuracy: 0.5312\n",
      "Loss: 0.5169, Accuracy: 0.6875\n",
      "Loss: 0.7039, Accuracy: 0.5312\n",
      "Loss: 0.5147, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6600\n",
      "epoch: 85\n",
      "Loss: 0.4600, Accuracy: 0.8125\n",
      "Loss: 0.6490, Accuracy: 0.5938\n",
      "Loss: 0.6345, Accuracy: 0.5000\n",
      "Loss: 0.4978, Accuracy: 0.7188\n",
      "Loss: 0.5939, Accuracy: 0.5625\n",
      "Loss: 0.5371, Accuracy: 0.5312\n",
      "Loss: 0.6090, Accuracy: 0.6562\n",
      "Loss: 0.5644, Accuracy: 0.6250\n",
      "Loss: 0.5037, Accuracy: 0.6875\n",
      "Loss: 0.5401, Accuracy: 0.7188\n",
      "Loss: 0.5527, Accuracy: 0.6250\n",
      "Loss: 0.4855, Accuracy: 0.7188\n",
      "Test Accuracy: 0.6650\n",
      "epoch: 86\n",
      "Loss: 0.5802, Accuracy: 0.6562\n",
      "Loss: 0.5656, Accuracy: 0.6875\n",
      "Loss: 0.4858, Accuracy: 0.6562\n",
      "Loss: 0.4678, Accuracy: 0.7500\n",
      "Loss: 0.5445, Accuracy: 0.6562\n",
      "Loss: 0.5604, Accuracy: 0.6875\n",
      "Loss: 0.5150, Accuracy: 0.6250\n",
      "Loss: 0.5116, Accuracy: 0.6875\n",
      "Loss: 0.5878, Accuracy: 0.5938\n",
      "Loss: 0.4720, Accuracy: 0.6250\n",
      "Loss: 0.4966, Accuracy: 0.6562\n",
      "Loss: 0.4906, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6600\n",
      "epoch: 87\n",
      "Loss: 0.5072, Accuracy: 0.7500\n",
      "Loss: 0.6051, Accuracy: 0.5938\n",
      "Loss: 0.5032, Accuracy: 0.6875\n",
      "Loss: 0.5274, Accuracy: 0.5625\n",
      "Loss: 0.4946, Accuracy: 0.5625\n",
      "Loss: 0.6431, Accuracy: 0.7188\n",
      "Loss: 0.6081, Accuracy: 0.5938\n",
      "Loss: 0.4230, Accuracy: 0.7188\n",
      "Loss: 0.6209, Accuracy: 0.5625\n",
      "Loss: 0.4705, Accuracy: 0.8125\n",
      "Loss: 0.5397, Accuracy: 0.5938\n",
      "Loss: 0.5333, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6650\n",
      "epoch: 88\n",
      "Loss: 0.4893, Accuracy: 0.7500\n",
      "Loss: 0.6001, Accuracy: 0.5938\n",
      "Loss: 0.4888, Accuracy: 0.7188\n",
      "Loss: 0.5204, Accuracy: 0.7812\n",
      "Loss: 0.5028, Accuracy: 0.6250\n",
      "Loss: 0.5411, Accuracy: 0.5625\n",
      "Loss: 0.5334, Accuracy: 0.5312\n",
      "Loss: 0.5008, Accuracy: 0.7812\n",
      "Loss: 0.5340, Accuracy: 0.6875\n",
      "Loss: 0.5549, Accuracy: 0.6875\n",
      "Loss: 0.4846, Accuracy: 0.5938\n",
      "Loss: 0.5844, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6850\n",
      "epoch: 89\n",
      "Loss: 0.5097, Accuracy: 0.7188\n",
      "Loss: 0.5818, Accuracy: 0.5625\n",
      "Loss: 0.5140, Accuracy: 0.5625\n",
      "Loss: 0.5175, Accuracy: 0.6875\n",
      "Loss: 0.5636, Accuracy: 0.6562\n",
      "Loss: 0.6161, Accuracy: 0.6250\n",
      "Loss: 0.6010, Accuracy: 0.5938\n",
      "Loss: 0.3477, Accuracy: 0.7500\n",
      "Loss: 0.4336, Accuracy: 0.6250\n",
      "Loss: 0.4271, Accuracy: 0.6875\n",
      "Loss: 0.5533, Accuracy: 0.5312\n",
      "Loss: 0.5176, Accuracy: 0.6250\n",
      "Test Accuracy: 0.6700\n",
      "epoch: 90\n",
      "Loss: 0.5327, Accuracy: 0.5625\n",
      "Loss: 0.5034, Accuracy: 0.7812\n",
      "Loss: 0.5672, Accuracy: 0.6562\n",
      "Loss: 0.5564, Accuracy: 0.6875\n",
      "Loss: 0.5980, Accuracy: 0.6562\n",
      "Loss: 0.6034, Accuracy: 0.5000\n",
      "Loss: 0.5207, Accuracy: 0.8125\n",
      "Loss: 0.5155, Accuracy: 0.6875\n",
      "Loss: 0.5777, Accuracy: 0.4375\n",
      "Loss: 0.5073, Accuracy: 0.6562\n",
      "Loss: 0.4883, Accuracy: 0.7188\n",
      "Loss: 0.5283, Accuracy: 0.5625\n",
      "Test Accuracy: 0.6850\n",
      "epoch: 91\n",
      "Loss: 0.4375, Accuracy: 0.6562\n",
      "Loss: 0.5298, Accuracy: 0.7812\n",
      "Loss: 0.5316, Accuracy: 0.5000\n",
      "Loss: 0.5742, Accuracy: 0.5000\n",
      "Loss: 0.5280, Accuracy: 0.6875\n",
      "Loss: 0.4458, Accuracy: 0.7500\n",
      "Loss: 0.4920, Accuracy: 0.6250\n",
      "Loss: 0.4920, Accuracy: 0.5938\n",
      "Loss: 0.4396, Accuracy: 0.7500\n",
      "Loss: 0.4576, Accuracy: 0.6875\n",
      "Loss: 0.4975, Accuracy: 0.7500\n",
      "Loss: 0.5059, Accuracy: 0.7500\n",
      "Test Accuracy: 0.6750\n",
      "epoch: 92\n",
      "Loss: 0.5508, Accuracy: 0.7500\n",
      "Loss: 0.6071, Accuracy: 0.5938\n",
      "Loss: 0.5252, Accuracy: 0.6875\n",
      "Loss: 0.5847, Accuracy: 0.6250\n",
      "Loss: 0.5589, Accuracy: 0.5938\n",
      "Loss: 0.5213, Accuracy: 0.7188\n",
      "Loss: 0.6102, Accuracy: 0.5625\n",
      "Loss: 0.5644, Accuracy: 0.7188\n",
      "Loss: 0.5422, Accuracy: 0.7500\n",
      "Loss: 0.5384, Accuracy: 0.6250\n",
      "Loss: 0.3721, Accuracy: 0.8125\n",
      "Loss: 0.5242, Accuracy: 0.7812\n",
      "Test Accuracy: 0.6650\n",
      "epoch: 93\n",
      "Loss: 0.4274, Accuracy: 0.6562\n",
      "Loss: 0.4538, Accuracy: 0.7812\n",
      "Loss: 0.4670, Accuracy: 0.8438\n",
      "Loss: 0.5959, Accuracy: 0.5625\n",
      "Loss: 0.5802, Accuracy: 0.6250\n",
      "Loss: 0.5042, Accuracy: 0.6875\n",
      "Loss: 0.4802, Accuracy: 0.6875\n",
      "Loss: 0.4980, Accuracy: 0.6562\n",
      "Loss: 0.5616, Accuracy: 0.7188\n",
      "Loss: 0.5661, Accuracy: 0.5938\n",
      "Loss: 0.4531, Accuracy: 0.7500\n",
      "Loss: 0.5649, Accuracy: 0.6250\n",
      "Test Accuracy: 0.6750\n",
      "epoch: 94\n",
      "Loss: 0.5175, Accuracy: 0.6875\n",
      "Loss: 0.4920, Accuracy: 0.8125\n",
      "Loss: 0.4624, Accuracy: 0.7188\n",
      "Loss: 0.6153, Accuracy: 0.5000\n",
      "Loss: 0.5037, Accuracy: 0.6875\n",
      "Loss: 0.5353, Accuracy: 0.5625\n",
      "Loss: 0.5534, Accuracy: 0.6562\n",
      "Loss: 0.5429, Accuracy: 0.7188\n",
      "Loss: 0.4320, Accuracy: 0.7812\n",
      "Loss: 0.4118, Accuracy: 0.7500\n",
      "Loss: 0.6066, Accuracy: 0.6562\n",
      "Loss: 0.5218, Accuracy: 0.5938\n",
      "Test Accuracy: 0.6750\n",
      "epoch: 95\n",
      "Loss: 0.4696, Accuracy: 0.7812\n",
      "Loss: 0.5021, Accuracy: 0.7500\n",
      "Loss: 0.4581, Accuracy: 0.7188\n",
      "Loss: 0.4062, Accuracy: 0.7812\n",
      "Loss: 0.4448, Accuracy: 0.6875\n",
      "Loss: 0.5532, Accuracy: 0.5625\n",
      "Loss: 0.4544, Accuracy: 0.7188\n",
      "Loss: 0.6474, Accuracy: 0.6250\n",
      "Loss: 0.4677, Accuracy: 0.7188\n",
      "Loss: 0.5093, Accuracy: 0.6250\n",
      "Loss: 0.5051, Accuracy: 0.8125\n",
      "Loss: 0.4939, Accuracy: 0.8125\n",
      "Test Accuracy: 0.6750\n",
      "epoch: 96\n",
      "Loss: 0.4545, Accuracy: 0.7188\n",
      "Loss: 0.5362, Accuracy: 0.6562\n",
      "Loss: 0.5686, Accuracy: 0.5312\n",
      "Loss: 0.4993, Accuracy: 0.7188\n",
      "Loss: 0.3530, Accuracy: 0.7812\n",
      "Loss: 0.4961, Accuracy: 0.6875\n",
      "Loss: 0.5284, Accuracy: 0.6562\n",
      "Loss: 0.5139, Accuracy: 0.6875\n",
      "Loss: 0.5404, Accuracy: 0.6875\n",
      "Loss: 0.5278, Accuracy: 0.7188\n",
      "Loss: 0.5164, Accuracy: 0.7500\n",
      "Loss: 0.5807, Accuracy: 0.7500\n",
      "Test Accuracy: 0.6750\n",
      "epoch: 97\n",
      "Loss: 0.4330, Accuracy: 0.7500\n",
      "Loss: 0.4304, Accuracy: 0.7812\n",
      "Loss: 0.5580, Accuracy: 0.5625\n",
      "Loss: 0.5165, Accuracy: 0.7188\n",
      "Loss: 0.5194, Accuracy: 0.7812\n",
      "Loss: 0.5163, Accuracy: 0.7812\n",
      "Loss: 0.4410, Accuracy: 0.7812\n",
      "Loss: 0.4666, Accuracy: 0.7500\n",
      "Loss: 0.4589, Accuracy: 0.7188\n",
      "Loss: 0.3847, Accuracy: 0.8438\n",
      "Loss: 0.4770, Accuracy: 0.7188\n",
      "Loss: 0.6144, Accuracy: 0.4688\n",
      "Test Accuracy: 0.6650\n",
      "epoch: 98\n",
      "Loss: 0.5350, Accuracy: 0.6250\n",
      "Loss: 0.4639, Accuracy: 0.6875\n",
      "Loss: 0.5380, Accuracy: 0.6562\n",
      "Loss: 0.4465, Accuracy: 0.8125\n",
      "Loss: 0.4699, Accuracy: 0.6250\n",
      "Loss: 0.4726, Accuracy: 0.7500\n",
      "Loss: 0.5174, Accuracy: 0.6562\n",
      "Loss: 0.5903, Accuracy: 0.5938\n",
      "Loss: 0.4812, Accuracy: 0.6562\n",
      "Loss: 0.3552, Accuracy: 0.7812\n",
      "Loss: 0.5026, Accuracy: 0.7812\n",
      "Loss: 0.4592, Accuracy: 0.6875\n",
      "Test Accuracy: 0.6600\n",
      "epoch: 99\n",
      "Loss: 0.4258, Accuracy: 0.7188\n",
      "Loss: 0.5341, Accuracy: 0.6250\n",
      "Loss: 0.5367, Accuracy: 0.6562\n",
      "Loss: 0.4643, Accuracy: 0.7188\n",
      "Loss: 0.4746, Accuracy: 0.8125\n",
      "Loss: 0.5063, Accuracy: 0.6875\n",
      "Loss: 0.4077, Accuracy: 0.7188\n",
      "Loss: 0.5060, Accuracy: 0.6875\n",
      "Loss: 0.5217, Accuracy: 0.6875\n",
      "Loss: 0.5491, Accuracy: 0.5000\n",
      "Loss: 0.5514, Accuracy: 0.7500\n",
      "Loss: 0.4450, Accuracy: 0.7500\n",
      "Test Accuracy: 0.6650\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "epoch = 0\n",
    "\n",
    "while epoch < max_epochs:\n",
    "    print('epoch:', epoch)\n",
    "    n_batch = X_train.shape[0] // batch_sz\n",
    "    \n",
    "    for batch_i in range(n_batch):\n",
    "        # Get a mini-batch of data from X_train and y_train.\n",
    "        X_batch, y_batch = get_random_mini_batch(X_train, y_train, batch_sz)\n",
    "        \n",
    "        # One-hot encode y_true\n",
    "        oh_y_batch = np.eye(n_class)[y_batch]\n",
    "        \n",
    "        # Forward pass\n",
    "        probs = forward_pass(X_batch, y_batch, oh_y_batch)\n",
    "        \n",
    "        # Loss\n",
    "        loss = crossentropy.forward(probs, oh_y_batch)\n",
    "        \n",
    "        # Print accuracy and loss\n",
    "        y_preds = predictions(probs)\n",
    "        acc = accuracy(y_preds, y_batch)\n",
    "        print(f\"Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # Backward pass\n",
    "        backward_pass(probs, y_batch, oh_y_batch)\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "    \n",
    "    # Test the model on the test set\n",
    "    test_probs = forward_pass(X_test, y_test, np.eye(n_class)[y_test])\n",
    "    y_test_preds = predictions(test_probs)\n",
    "    current_accuracy = accuracy(y_test_preds, y_test)\n",
    "    print(f\"Test Accuracy: {current_accuracy:.4f}\")\n",
    "    \n",
    "    if current_accuracy >= 0.9:\n",
    "        print(\"Achieved desired accuracy! Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Loss: 0.5045, Accuracy: 0.88\n",
      "Loss: 0.5578, Accuracy: 0.88\n",
      "Loss: 0.4070, Accuracy: 1.00\n",
      "Loss: 0.4509, Accuracy: 0.88\n",
      "Loss: 0.3794, Accuracy: 0.88\n",
      "Loss: 0.4683, Accuracy: 0.97\n",
      "Loss: 0.4141, Accuracy: 0.91\n",
      "Loss: 0.5129, Accuracy: 0.88\n",
      "Loss: 0.4735, Accuracy: 0.81\n",
      "Loss: 0.5128, Accuracy: 0.88\n",
      "Loss: 0.4132, Accuracy: 0.94\n",
      "Loss: 0.4739, Accuracy: 0.91\n",
      "Test Accuracy: 0.8600\n",
      "epoch: 1\n",
      "Loss: 0.4697, Accuracy: 0.91\n",
      "Loss: 0.4230, Accuracy: 0.91\n",
      "Loss: 0.5195, Accuracy: 0.91\n",
      "Loss: 0.4001, Accuracy: 0.97\n",
      "Loss: 0.3453, Accuracy: 0.91\n",
      "Loss: 0.4260, Accuracy: 0.97\n",
      "Loss: 0.5353, Accuracy: 0.91\n",
      "Loss: 0.3876, Accuracy: 0.94\n",
      "Loss: 0.4964, Accuracy: 0.81\n",
      "Loss: 0.4466, Accuracy: 0.94\n",
      "Loss: 0.4676, Accuracy: 0.94\n",
      "Loss: 0.3047, Accuracy: 0.97\n",
      "Test Accuracy: 0.9150\n",
      "Achieved desired accuracy. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "current_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    print('epoch:', epoch)\n",
    "    n_batch = X_train.shape[0] // batch_sz\n",
    "    for batch_i in range(n_batch):\n",
    "        # Get a random-mini-batch of data from X_train and y_train.\n",
    "        X_batch, y_batch = get_random_mini_batch(X_train, y_train, batch_sz)\n",
    "            \n",
    "        # One-hot encode y_true\n",
    "        Oh_y_batch = np.eye(n_class)[y_batch]\n",
    "            \n",
    "        # Forward pass\n",
    "        probs = forward_pass(X_batch, y_batch, Oh_y_batch)\n",
    "            \n",
    "        # Loss\n",
    "        loss = crossentropy.forward(probs, Oh_y_batch)\n",
    "        # print accuracy\n",
    "        y_preds = predictions(probs)\n",
    "        acc = accuracy(y_preds, y_batch)\n",
    "        print(f\"Loss: {loss:.4f}, Accuracy: {acc:.2f}\")\n",
    "        # Backward pass\n",
    "        backward_pass(probs, y_batch, Oh_y_batch)\n",
    "            \n",
    "        # Update the weights\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "\n",
    "    test_probs = forward_pass(X_test, y_test, np.eye(n_class)[y_test])\n",
    "    y_test_preds = predictions(test_probs)\n",
    "    test_accuracy = accuracy(y_test_preds, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "    if test_accuracy >= 0.9:\n",
    "        print(\"Achieved desired accuracy. Stopping training.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9050\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "while test_accuracy < 0.9:\n",
    "    test_probs = forward_pass(X_test, y_test, np.eye(n_class)[y_test])\n",
    "    y_test_preds = predictions(test_probs)\n",
    "    test_accuracy = accuracy(y_test_preds, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "    if test_accuracy >= 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
